# Radcom Traffic Classification System ðŸš€

This project is a high-performance, scalable machine learning system designed to classify encrypted network traffic.
---

## ðŸ§  Machine Learning Approach

### Algorithm
The system utilizes the **Random Forest Classifier** for both tasks:

- **App Classification**:  
  Trained with **200 estimators** and a **maximum depth of 25** to handle the high complexity of 128 target classes.

- **Attribution Classification**:  
  Trained with **100 estimators**, using all available CPU cores (`n_jobs=-1`) for efficient processing.

### Preprocessing & Feature Engineering
To ensure the model generalizes across different networks and avoids overfitting on identity data, the following steps are performed:

- **Identity Removal**:  
  Columns such as `Source_IP`, `Destination_IP`, and `Timestamp` are dropped to focus purely on behavioral patterns.

- **Protocol Encoding**:  
  Network protocols are encoded numerically:
  - TCP â†’ `6`
  - UDP â†’ `17`

- **Feature Alignment**:  
  During inference, incoming data is reindexed to match the exact feature order used during training, with missing values filled with zeros.

---

## ðŸ—ï¸ System Architecture

The project follows a **decoupled microservices architecture** designed to handle high-traffic *hexabyte-scale* workloads through horizontal scaling.

- **FastAPI**  
  REST API gateway serving as the entry point for inference requests using the OpenAPI standard.

- **Celery & Redis**  
  Asynchronous task queue that prevents API blocking during heavy ML computations, enabling massive scale.

- **Streamlit**  
  User-friendly web interface for file uploads, real-time status polling, and result visualization.

- **Docker**  
  All components are containerized to ensure environment consistency across development and production.

---

## ðŸš€ Getting Started

### 1. Model Training & Testing

> âš ï¸ Models are **not included** in the source submission.  
> You must generate them before starting the Docker services.

The trained models will be saved under the `models/` directory.

#### Train the models
```bash
# Generate the Application Model
python3 src/train_app.py

# Generate the Attribution Model
python3 src/train_att.py
```

#### Test the models
Evaluate accuracy and view classification reports:
```bash
python3 src/test_models.py
```

---

### 2. Activating the Docker Environment

Ensure your `models/` directory contains the generated `.pkl` files before launching the containers.

```bash
# Build and start all services
sudo docker compose up --build
```

---

### 3. Accessing the System

Once the containers are running:

- **Streamlit Web UI**  
  http://localhost:8501  
  Upload your validation CSV and receive results with the prediction column.

- **FastAPI Docs (OpenAPI)**  
  http://localhost:8000/docs  
  Interact directly with the REST API endpoints.

---

## ðŸ“‚ Project Structure

```
â”œâ”€â”€ Dockerfile.app              # FastAPI inference service container
â”œâ”€â”€ Dockerfile.worker           # Celery worker container for background ML tasks
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ docker-compose.yml          # Full stack orchestration (API, Worker, Redis, UI)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ data                        # Training, testing, and validation datasets
â”‚   â”œâ”€â”€ APP-1                   # Application classification datasets
â”‚   â”‚   â”œâ”€â”€ radcom_app_train.csv
â”‚   â”‚   â”œâ”€â”€ radcom_app_test.csv
â”‚   â”‚   â””â”€â”€ radcom_app_val_without_labels.csv
â”‚   â””â”€â”€ attribution             # Traffic attribution datasets
â”‚       â”œâ”€â”€ radcom_att_train.csv
â”‚       â”œâ”€â”€ radcom_att_test.csv
â”‚       â””â”€â”€ radcom__att_val_without_labels.csv
â”‚
â”œâ”€â”€ models                      # Trained models and feature definitions
â”‚   â”œâ”€â”€ model_app.pkl           # Random Forest model for application classification
â”‚   â”œâ”€â”€ model_att.pkl           # Random Forest model for attribution classification
â”‚   â”œâ”€â”€ app_features.pkl        # Ordered feature list for app inference
â”‚   â””â”€â”€ att_features.pkl        # Ordered feature list for attribution inference
â”‚
â”œâ”€â”€ predictions                 # Output prediction results
â”‚   â”œâ”€â”€ app_predictions.csv
â”‚   â””â”€â”€ att_predictions.csv
â”‚
â”œâ”€â”€ src                         # Core application source code
â”‚   â”œâ”€â”€ api.py                  # FastAPI server and REST endpoints
â”‚   â”œâ”€â”€ tasks.py                # Celery task definitions for asynchronous inference
â”‚   â”œâ”€â”€ worker.py               # Celery worker initialization and model loading
â”‚   â”œâ”€â”€ streamlit_app.py        # Streamlit web UI for file upload and result visualization
â”‚   â””â”€â”€ utils.py                # Shared preprocessing and feature-engineering utilities
â”‚
â”œâ”€â”€ train                       # Model training scripts
â”‚   â”œâ”€â”€ train_app.py            # Application classifier training pipeline
â”‚   â””â”€â”€ train_att.py            # Attribution classifier training pipeline
â”‚
â””â”€â”€ test_models.py              # Model evaluation and validation reports
```
